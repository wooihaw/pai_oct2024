{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Loading Built-in Dataset](#loading-builtin-data)\n",
    "* [Loading External Data](#loading-external-data)\n",
    "* [Statistical Summary and Class Breakdown](#statistical-summary)\n",
    "* [Dropping Rows with Missing Values](#dropping-rows)\n",
    "* [Data Imputation](#data-imputation)\n",
    "* [Categorical Data](#categorical-data)\n",
    "* [Min-max Scaling](#min-max-scaling)\n",
    "* [Standard Scaling](#standard-scaling)\n",
    "* [Robust Scaling](#robust-scaling)\n",
    "* [Feature Engineering](#feature-engineering)\n",
    "* [Univariate Selection](#univariate-selection)\n",
    "* [Recursive Feature Elimination](#recursive-feature-elimination)\n",
    "* [Hold-out Validation](#hold-out-validation)\n",
    "* [Cross Validation](#cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "%matplotlib inline\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Built-in Dataset <a class=\"anchor\" id=\"loading-builtin-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary module from scikit-learn\n",
    "from sklearn.datasets import load_iris\n",
    "# Load the Iris dataset\n",
    "dataset = load_iris()\n",
    "# Extract features and target\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "# Display feature names and target names\n",
    "print(\"Feature Names:\", dataset.feature_names)\n",
    "print(\"Target Names:\", dataset.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading External Data <a class=\"anchor\" id=\"loading-external-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV file\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "header = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv('pima-indians-diabetes.data.csv', names=header)\n",
    "# Separate data into features and target\n",
    "X = df.drop(columns=['class'])\n",
    "y = df['class']\n",
    "print(df.shape, X.shape, y.shape) # print the dimension of the dataframe, X & y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Summary and Class Breakdown <a class=\"anchor\" id=\"statistical-summary\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print statistical summary and class breakdown\n",
    "from pandas import read_csv\n",
    "header = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv('pima-indians-diabetes.data.csv', names=header)\n",
    "print(df.describe()) # print the statistical summary of the data\n",
    "class_counts = df.groupby('class').size()\n",
    "print(class_counts) # print the class breakdown of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Rows with Missing Values <a class=\"anchor\" id=\"dropping-rows\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values by dropping data samples with missing values\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame({'Age': [17, 23, 0, 38, 54, 67, 32],\n",
    "                  'Height': [160, 172, 150, 165, 163, 158, 175],\n",
    "                  'Weight':[50, 68, 43, 52, 47, 49, 0]})\n",
    "df = df.replace({0: np.nan}) # replace missing value (0) with NaN\n",
    "print(df)\n",
    "print(df.isnull().sum())\n",
    "df = df.dropna() # drop rows with NaN\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imputation <a class=\"anchor\" id=\"data-imputation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Handling missing values by imputing missing values with statistic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame({'Age': [17, 23, 0, 38, 54, 67, 32],\n",
    "                  'Height': [160, 172, 150, 165, 163, 158, 175],\n",
    "                  'Weight':[50, 68, 43, 52, 47, 49, 0]})\n",
    "df = df.replace({0: np.nan})\n",
    "df = df.fillna({'Age': df['Age'].median(), 'Weight': df['Weight'].mean()}) # replace NaN with statistics\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data <a class=\"anchor\" id=\"categorical-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling categorical data\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'year':[2015, 2017, 2013, 2018, 2020], \n",
    "                  'maker':['Toyota', 'Honda', 'Perodua', 'Hyundai', 'Toyota'],\n",
    "                  'engine':[1.5, 1.8, 1.3, 1.6, 1.8],\n",
    "                  'review':['moderate', 'good', 'poor', 'moderate', 'good']})\n",
    "mapping = {'poor':1, 'moderate':2, 'good':3}\n",
    "df['review'] = df['review'].map(mapping) # encode ordinal data\n",
    "df = pd.get_dummies(df, columns=['maker']) # encode nominal data\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-max Scaling <a class=\"anchor\" id=\"min-max-scaling\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data (between 0 and 1)\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "header = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv('pima-indians-diabetes.data.csv', names=header)\n",
    "# Separate into features and target\n",
    "X = df.drop(columns=['class'])\n",
    "y = df['class']\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X)\n",
    "scaledX = scaler.transform(X)\n",
    "# Check min and max of all column\n",
    "print(f'minimum={np.min(scaledX, axis=0)}, maximum={np.max(scaledX, axis=0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaling <a class=\"anchor\" id=\"standard-scaling\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data (0 mean, 1 stdev)\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas import read_csv\n",
    "header = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv('pima-indians-diabetes.data.csv', names=header)\n",
    "# Separate into features and target\n",
    "X = df.drop(columns=['class'])\n",
    "y = df['class']\n",
    "scaler = StandardScaler()\n",
    "scaledX = scaler.fit_transform(X)\n",
    "# Check mean and standard deviation of all columns\n",
    "print(f'mean={np.mean(scaledX, axis=0)}, variance={np.var(scaledX, axis=0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Scaling <a class=\"anchor\" id=\"robust-scaling\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust scaling (0 median, 1 IQR)\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from pandas import read_csv\n",
    "header = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv('pima-indians-diabetes.data.csv', names=header)\n",
    "# Separate into features and target\n",
    "X = df.drop(columns=['class'])\n",
    "y = df['class']\n",
    "scaler = RobustScaler()\n",
    "scaledX = scaler.fit_transform(X)\n",
    "# Check median and IQR of all columns\n",
    "q3, q1 = np.percentile(scaledX, [75 ,25], axis=0)\n",
    "print(f'median={np.median(scaledX, axis=0)}, IQR={q3-q1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering <a class=\"anchor\" id=\"feature-engineering\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create 2 new features\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "header = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv('pima-indians-diabetes.data.csv', names=header)\n",
    "win_size = 3\n",
    "df['new_feature1'] = df['plas'] + df['pres'] # new feature 1\n",
    "df['new_feature2'] = df['mass'].rolling(win_size).mean() # new feature 2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Selection <a class=\"anchor\" id=\"univariate-selection\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature Selection with Univariate Selection\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "# load data\n",
    "header = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv('pima-indians-diabetes.data.csv', names=header)\n",
    "X = df.drop(columns=['class'])\n",
    "y = df['class']\n",
    "selector = SelectKBest(k=4)\n",
    "features = selector.fit_transform(X, y)\n",
    "selected = selector.get_support()\n",
    "# Show selected features\n",
    "print([header[i] for i, j in enumerate(selected) if j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination <a class=\"anchor\" id=\"recursive-feature-elimination\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature Selection with RFE\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "header = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(filename, names=header)\n",
    "X = df.drop(columns=['class'])\n",
    "y = df['class']\n",
    "model = DecisionTreeClassifier()\n",
    "rfe = RFE(model, n_features_to_select=4)\n",
    "features = rfe.fit_transform(X, y)\n",
    "selected = rfe.get_support()\n",
    "# Show selected features\n",
    "print([header[i] for i, j in enumerate(selected) if j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold-out Validation <a class=\"anchor\" id=\"hold-out-validation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using a train and a test set\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "header = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv('pima-indians-diabetes.data.csv', names=header)\n",
    "X = df.drop(columns=['class'])\n",
    "y = df['class']\n",
    "X_train, X_test, y_train, y_test = split(X, y, test_size=0.33, random_state=42)\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)\n",
    "print(f\"Accuracy: {result:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation <a class=\"anchor\" id=\"cross-validation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate using Cross Validation\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "header = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv('pima-indians-diabetes.data.csv', names=header)\n",
    "X = df.drop(columns=['class'])\n",
    "y = df['class']\n",
    "model = KNeighborsClassifier()\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "results = cross_val_score(model, X, y, cv=kfold)\n",
    "print(f\"Accuracy: {results.mean():.2%} ({results.std():.2%})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
